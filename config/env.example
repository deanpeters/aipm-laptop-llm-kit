# AIPM Laptop LLM Kit Environment Variables
# Copy this file to .env and customize as needed

# Primary Local LLM Configuration (Ollama - Recommended)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_API_KEY=local-ollama-key
OLLAMA_MODEL_NAME=phi3:mini
OLLAMA_DOCKER_URL=http://host.docker.internal:11434/v1
OLLAMA_PORT=11434

# Docker-accessible Ollama endpoint (for n8n, LangFlow, etc.)
# Note: host.docker.internal requires extra_hosts in docker-compose.yml
OLLAMA_STORAGE=./storage/ollama

# Alternative Local Models (configure as needed)
# === LM STUDIO CONFIGURATION (Backup) ===
# LM Studio local server (backup to Ollama)
LLM_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=local-lmstudio-key
LLM_MODEL_NAME=phi-3-mini-4k-instruct
LLM_DOCKER_URL=http://host.docker.internal:1234/v1

# Common Ollama models (uncomment to use)
# OLLAMA_MODEL_NAME=llama3.2:3b
# OLLAMA_MODEL_NAME=phi3:mini
# OLLAMA_MODEL_NAME=codellama:7b
# OLLAMA_MODEL_NAME=mistral:7b

# Custom Fine-tuned Models
# PM_SPECIALIST_URL=http://localhost:1234/v1
# PM_SPECIALIST_KEY=local-pm-specialist-key  
# PM_SPECIALIST_MODEL=pm-assistant-v2

# Code-Specialized Models
# CODE_SPECIALIST_URL=http://localhost:1235/v1
# CODE_SPECIALIST_KEY=local-code-specialist-key
# CODE_SPECIALIST_MODEL=deepseek-coder-v2

# LangFlow-specific Environment Variables
# LANGFLOW_LLM_URL=http://host.docker.internal:1234/v1
# LANGFLOW_LLM_KEY=sk-local-key
# LANGFLOW_LLM_MODEL=phi-3-mini-4k-instruct

# Service ports
ANYTHINGLLM_PORT=3001
N8N_PORT=5678
PRIVATEGPT_PORT=8001
LANGFLOW_PORT=7860
OLLAMA_WEBUI_PORT=8080

# Storage paths (relative to project root)
ANYTHINGLLM_STORAGE=./storage/anythingllm
N8N_STORAGE=./storage/n8n
PRIVATEGPT_STORAGE=./storage/privategpt
LANGFLOW_STORAGE=./storage/langflow
OLLAMA_WEBUI_STORAGE=./storage/ollama-webui

# Project configuration
SOV_STACK_HOME=/path/to/aipm-laptop-llm-kit

# Optional: Slack webhook for n8n notifications
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Optional: Continue.dev config path
# CONTINUE_CONFIG=/path/to/config/continue.json

# External API Keys (for hybrid workflows)
# OPENAI_API_KEY=sk-your-openai-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# GOOGLE_API_KEY=your-google-ai-key-here

# === UNIVERSAL PROVIDER SELECTION ===
# Choose your primary local LLM provider (ollama recommended)
DEFAULT_LLM_PROVIDER=ollama    # ollama, lmstudio, openai, anthropic
BACKUP_LLM_PROVIDER=lmstudio   # fallback if primary unavailable

# Universal variables that auto-switch based on DEFAULT_LLM_PROVIDER
# These are automatically set - no need to edit manually
UNIVERSAL_LLM_URL=${OLLAMA_BASE_URL}
UNIVERSAL_LLM_DOCKER_URL=${OLLAMA_DOCKER_URL}  
UNIVERSAL_LLM_API_KEY=${OLLAMA_API_KEY}
UNIVERSAL_LLM_MODEL=${OLLAMA_MODEL_NAME}