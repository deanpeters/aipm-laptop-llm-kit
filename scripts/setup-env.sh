#!/bin/bash
set -e

# Environment Setup Script for AIPM Laptop LLM Kit
# Sets up environment variables with guarded blocks

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
GUARD_MARKER="aipm-laptop-llm-kit"

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log() {
    echo -e "$1"
}

success() {
    echo -e "${GREEN}✓ $1${NC}"
}

warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}

# Create .env file if it doesn't exist
create_env_file() {
    local env_file="$PROJECT_ROOT/.env"
    local env_example="$PROJECT_ROOT/config/env.example"
    
    if [[ ! -f "$env_file" ]]; then
        if [[ -f "$env_example" ]]; then
            cp "$env_example" "$env_file"
            success "Created .env file from template"
        else
            # Create basic .env file with API key patterns
            cat > "$env_file" << EOF
# AIPM Laptop LLM Kit Environment Variables
# Generated by installer on $(date)

# Local LLM Configuration (LM Studio)
LLM_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=local-lmstudio-key
LLM_MODEL_NAME=phi-3-mini-4k-instruct

# Docker-accessible LLM endpoint (for n8n, LangFlow, etc.)
LLM_DOCKER_URL=http://host.docker.internal:1234/v1

# Ollama Configuration (alternative to LM Studio)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_API_KEY=local-ollama-key
OLLAMA_MODEL_NAME=phi3:mini
OLLAMA_DOCKER_URL=http://host.docker.internal:11434/v1

# Service ports
ANYTHINGLLM_PORT=3001
N8N_PORT=5678
PRIVATEGPT_PORT=8001
LANGFLOW_PORT=7860
OLLAMA_WEBUI_PORT=8080

# Storage paths (relative to project root)
ANYTHINGLLM_STORAGE=./storage/anythingllm
N8N_STORAGE=./storage/n8n
PRIVATEGPT_STORAGE=./storage/privategpt
LANGFLOW_STORAGE=./storage/langflow
OLLAMA_WEBUI_STORAGE=./storage/ollama-webui

# Project configuration
SOV_STACK_HOME=$PROJECT_ROOT

# Example: Uncomment and customize for specialized models
# PM_SPECIALIST_URL=http://localhost:1234/v1
# PM_SPECIALIST_KEY=local-pm-specialist-key  
# PM_SPECIALIST_MODEL=pm-assistant-v2

# Example: External API keys for hybrid workflows
# OPENAI_API_KEY=sk-your-openai-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
EOF
            success "Created basic .env file"
        fi
    else
        log ".env file already exists"
    fi
}

# Add guarded block to shell config
add_to_shell_config() {
    local shell_config="$1"
    local shell_name="$2"
    
    if [[ ! -f "$shell_config" ]]; then
        touch "$shell_config"
    fi
    
    # Check if our block already exists
    if grep -q "# >>> $GUARD_MARKER >>>" "$shell_config"; then
        log "$shell_name config already contains our environment block"
        return
    fi
    
    log "Adding environment variables to $shell_name config..."
    
    cat >> "$shell_config" << EOF

# >>> $GUARD_MARKER >>>
# Local LLM Configuration
export LLM_BASE_URL=\${LLM_BASE_URL:-http://localhost:1234/v1}
export LLM_API_KEY=\${LLM_API_KEY:-local-lmstudio-key}
export LLM_MODEL_NAME=\${LLM_MODEL_NAME:-phi-3-mini-4k-instruct}
export LLM_DOCKER_URL=\${LLM_DOCKER_URL:-http://host.docker.internal:1234/v1}

# Ollama Configuration
export OLLAMA_BASE_URL=\${OLLAMA_BASE_URL:-http://localhost:11434/v1}
export OLLAMA_API_KEY=\${OLLAMA_API_KEY:-local-ollama-key}
export OLLAMA_MODEL_NAME=\${OLLAMA_MODEL_NAME:-phi3:mini}
export OLLAMA_DOCKER_URL=\${OLLAMA_DOCKER_URL:-http://host.docker.internal:11434/v1}

# Service Ports
export ANYTHINGLLM_PORT=\${ANYTHINGLLM_PORT:-3001}
export N8N_PORT=\${N8N_PORT:-5678}
export PRIVATEGPT_PORT=\${PRIVATEGPT_PORT:-8001}
export LANGFLOW_PORT=\${LANGFLOW_PORT:-7860}
export OLLAMA_WEBUI_PORT=\${OLLAMA_WEBUI_PORT:-8080}

# Storage Locations
export ANYTHINGLLM_STORAGE=\${ANYTHINGLLM_STORAGE:-$PROJECT_ROOT/storage/anythingllm}
export N8N_STORAGE=\${N8N_STORAGE:-$PROJECT_ROOT/storage/n8n}
export PRIVATEGPT_STORAGE=\${PRIVATEGPT_STORAGE:-$PROJECT_ROOT/storage/privategpt}
export LANGFLOW_STORAGE=\${LANGFLOW_STORAGE:-$PROJECT_ROOT/storage/langflow}
export OLLAMA_WEBUI_STORAGE=\${OLLAMA_WEBUI_STORAGE:-$PROJECT_ROOT/storage/ollama-webui}

# Project Configuration
export SOV_STACK_HOME=\${SOV_STACK_HOME:-$PROJECT_ROOT}
export SOV_PROFILE_MARK="$GUARD_MARKER-$(date +%s)"
# <<< $GUARD_MARKER <<<
EOF
    
    success "Added environment block to $shell_name"
}

# Setup shell environments
setup_shell_env() {
    # Detect available shells and add to their configs
    if [[ -n "$ZSH_VERSION" ]] || [[ "$SHELL" == *"zsh"* ]]; then
        add_to_shell_config "$HOME/.zshrc" "zsh"
    fi
    
    if [[ -n "$BASH_VERSION" ]] || [[ "$SHELL" == *"bash"* ]]; then
        add_to_shell_config "$HOME/.bashrc" "bash"
    fi
    
    # Always add to bash profile for login shells
    if [[ -f "$HOME/.bash_profile" ]] || [[ "$SHELL" == *"bash"* ]]; then
        add_to_shell_config "$HOME/.bash_profile" "bash profile"
    fi
    
    # Source current environment from .env
    if [[ -f "$PROJECT_ROOT/.env" ]]; then
        set -a  # automatically export all variables
        source "$PROJECT_ROOT/.env"
        set +a  # stop automatically exporting
        success "Loaded environment variables from .env"
    fi
}

# Create storage directories
create_storage_dirs() {
    local storage_base="$PROJECT_ROOT/storage"
    local dirs=(
        "anythingllm"
        "n8n"
        "privategpt"
        "langflow"
        "ollama-webui"
    )
    
    for dir in "${dirs[@]}"; do
        mkdir -p "$storage_base/$dir"
    done
    
    success "Created storage directories"
}

# Set macOS GUI environment (optional)
setup_macos_gui_env() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        log "Setting up macOS GUI environment variables..."
        
        # Set for current session
        launchctl setenv LLM_BASE_URL "${LLM_BASE_URL:-http://localhost:1234/v1}" 2>/dev/null || true
        launchctl setenv LLM_API_KEY "${LLM_API_KEY:-local-lmstudio-key}" 2>/dev/null || true
        launchctl setenv LLM_MODEL_NAME "${LLM_MODEL_NAME:-phi-3-mini-4k-instruct}" 2>/dev/null || true
        launchctl setenv LLM_DOCKER_URL "${LLM_DOCKER_URL:-http://host.docker.internal:1234/v1}" 2>/dev/null || true
        launchctl setenv ANYTHINGLLM_PORT "${ANYTHINGLLM_PORT:-3001}" 2>/dev/null || true
        launchctl setenv N8N_PORT "${N8N_PORT:-5678}" 2>/dev/null || true
        launchctl setenv SOV_STACK_HOME "$PROJECT_ROOT" 2>/dev/null || true
        
        warning "macOS GUI apps will see these env vars after you log out and back in"
    fi
}

# Print environment summary
print_env_summary() {
    log ""
    log "=== Environment Summary ==="
    log "Local LLM Configuration:"
    log "  LLM_BASE_URL: ${LLM_BASE_URL:-http://localhost:1234/v1}"
    log "  LLM_API_KEY: ${LLM_API_KEY:-local-lmstudio-key}"
    log "  LLM_MODEL_NAME: ${LLM_MODEL_NAME:-phi-3-mini-4k-instruct}"
    log "  LLM_DOCKER_URL: ${LLM_DOCKER_URL:-http://host.docker.internal:1234/v1}"
    log ""
    log "Service Ports:"
    log "  AnythingLLM: ${ANYTHINGLLM_PORT:-3001}"
    log "  n8n: ${N8N_PORT:-5678}"
    log "  LangFlow: ${LANGFLOW_PORT:-7860}"
    log ""
    log "Project Home: ${SOV_STACK_HOME:-$PROJECT_ROOT}"
    log ""
    success "These environment variables are now available for:"
    log "  • n8n workflows (use ={{$env.LLM_API_KEY}} syntax)"
    log "  • LangFlow components (use {LLM_MODEL_NAME} syntax)"  
    log "  • VS Code integrations"
    log "  • Custom scripts and tools"
    log ""
    warning "Please restart your terminal or run 'source ~/.bashrc' (or ~/.zshrc) to use these variables"
}

# Main function
main() {
    log "Setting up AIPM Laptop LLM Kit environment..."
    
    create_env_file
    setup_shell_env
    create_storage_dirs
    setup_macos_gui_env
    print_env_summary
    
    success "Environment setup complete!"
}

main "$@"